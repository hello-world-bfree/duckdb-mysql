# name: test/sql/federation_cost_model_calibration.test
# description: Validate cost model parameter calibration from multi-agent optimization analysis
# group: [sql]

#
# This test validates the findings from the C++ and SQL specialist conversation:
# 1. cpu_cost_per_row = 0.1 (matches MySQL's row_evaluate_cost)
# 2. local_filter_cost = 0.001 (DuckDB is ~100x faster than MySQL)
# 3. Network cost correctly modeled as primary bottleneck
# 4. index_lookup_cost = 0.5 (random I/O cost)
#
# NOTE: EXECUTE_ALL_LOCALLY strategy is currently disabled because the scanner
# does not implement local filter execution. The cost model chooses between
# PUSH_ALL_FILTERS and HYBRID strategies only.

require mysql_scanner

require-env MYSQL_TEST_DATABASE_AVAILABLE

statement ok
SET GLOBAL mysql_experimental_filter_pushdown=true;

statement ok
ATTACH 'host=localhost user=root port="0" database="mysqlscanner"' AS s1 (TYPE MYSQL_SCANNER)

# ==============================================================================
# Test Setup: Create tables for cost model validation
# ==============================================================================

statement ok
DROP TABLE IF EXISTS s1.cost_calibration;

statement ok
CREATE TABLE s1.cost_calibration(
    id INTEGER PRIMARY KEY,
    indexed_val INTEGER,
    non_indexed_val INTEGER,
    wide_data VARCHAR(200)
);

statement ok
CREATE INDEX idx_calibration_val ON s1.cost_calibration(indexed_val);

# Insert 10000 rows for meaningful cost comparison
# indexed_val has 100 distinct values (0-99), so selectivity per value = 1%
statement ok
INSERT INTO s1.cost_calibration
SELECT
    range as id,
    range % 100 as indexed_val,
    range % 1000 as non_indexed_val,
    REPEAT('x', 100) as wide_data
FROM range(10000);

# Force MySQL to update statistics
statement ok
CALL mysql_execute('s1', 'ANALYZE TABLE cost_calibration');

# ==============================================================================
# Scenario 1: Point Lookup (0.01% selectivity) - SHOULD PUSH
# Cost analysis with new parameters:
# - Push: mysql_overhead(10) + cpu(0.1*1) + index_io(0.5) + network(1*100*0.00001) ≈ 10.6
# - Local: mysql_overhead(10) + seq_scan + network(10000*100*0.00001) + local_filter ≈ 20+
# Point lookups should always push to MySQL
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE id = 5000
----
1

query II
SELECT id, indexed_val FROM s1.cost_calibration WHERE id = 9999
----
9999	99

# ==============================================================================
# Scenario 2: Indexed Range (1% selectivity) - SHOULD PUSH
# indexed_val = 50 returns 100 rows out of 10000
# Cost analysis:
# - Push: 10 + 0.1*100 + 0.5*100 + 100*100*0.00001 = 10 + 10 + 50 + 0.1 ≈ 70
# - Local: 10 + seq_scan + 10000*100*0.00001 + 10000*0.001 = 10 + X + 10 + 10 ≈ 30+
# With index, pushing is more efficient
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val = 50
----
100

query I
SELECT SUM(id) FROM s1.cost_calibration WHERE indexed_val = 99
----
504900

# ==============================================================================
# Scenario 3: Non-indexed Medium Selectivity (10% selectivity)
# Range 0-9 returns 1000 rows (10% of data)
# Cost analysis:
# - Push: 10 + 0.1*1000 + seq_scan + 1000*100*0.00001 = 10 + 100 + X + 1 ≈ 111+
# - Local: 10 + seq_scan + 10000*100*0.00001 + 10000*0.001 = 10 + X + 10 + 10 ≈ 30+
# With 10% selectivity and no index, LOCAL may be cheaper
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val < 10
----
1000

# ==============================================================================
# Scenario 4: Low Selectivity (50%+) - SHOULD FAVOR LOCAL
# non_indexed_val < 500 returns ~50% of rows
# Transferring all rows + local filter is competitive with pushing
# The key insight: DuckDB's 100x faster filtering matters at scale
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE non_indexed_val < 500
----
5000

# ==============================================================================
# Scenario 5: Compound Filters - Test Hybrid Decision
# Combine highly selective + less selective filters
# indexed_val = 50 (1%) AND non_indexed_val > 500 (~50% reduction)
# Should push the indexed filter, possibly evaluate non-indexed locally
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val = 50 AND non_indexed_val >= 500
----
50

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val = 50 AND id > 5000
----
50

# ==============================================================================
# Scenario 6: Wide Rows - Network Cost Dominance
# With 200-byte wide rows, network cost becomes more significant
# This validates network_cost_per_byte = 0.00001 is reasonable
# ==============================================================================

query I
SELECT LENGTH(wide_data) FROM s1.cost_calibration WHERE id = 1
----
100

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val = 0 AND LENGTH(wide_data) > 50
----
100

# ==============================================================================
# Scenario 7: Multiple IN Values - Selectivity Estimation
# IN (1, 2, 3) has 3% selectivity (300 rows)
# Should still favor pushing due to index usage
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val IN (1, 2, 3)
----
300

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val IN (10, 20, 30, 40, 50)
----
500

# ==============================================================================
# Scenario 8: Very Large Result Set - Local Processing Wins
# When selectivity is very low (returning most rows), local filtering
# with DuckDB's vectorized execution should be competitive
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val >= 0
----
10000

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE indexed_val IS NOT NULL
----
10000

# ==============================================================================
# Scenario 9: Primary Key Range Scan
# id BETWEEN 1000 AND 1100 returns 101 rows (1%)
# Should push due to PK index
# ==============================================================================

query I
SELECT COUNT(*) FROM s1.cost_calibration WHERE id BETWEEN 1000 AND 1100
----
101

query II
SELECT MIN(id), MAX(id) FROM s1.cost_calibration WHERE id >= 9900
----
9900	9999

# ==============================================================================
# Scenario 10: Correctness Validation
# Ensure all query results are correct regardless of execution strategy
# ==============================================================================

query I
SELECT SUM(indexed_val) FROM s1.cost_calibration WHERE id < 100
----
4950

query I
SELECT AVG(non_indexed_val) FROM s1.cost_calibration WHERE indexed_val = 0
----
450.0

query I
SELECT COUNT(DISTINCT indexed_val) FROM s1.cost_calibration
----
100

# ==============================================================================
# Cleanup
# ==============================================================================

statement ok
DROP TABLE IF EXISTS s1.cost_calibration;
